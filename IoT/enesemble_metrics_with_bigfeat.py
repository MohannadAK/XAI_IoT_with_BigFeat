#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Ensemble Learning with BigFeat Comparison

This script compares the performance of ensemble models with and without
features generated by BigFeat for IoT device datasets.
Uses a local installation of BigFeat instead of the PyPi package.
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier, StackingClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.exceptions import ConvergenceWarning
import warnings

# Filter some common warnings
warnings.filterwarnings("ignore", category=ConvergenceWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message="DataFrame concatenation with empty or all-NA entries is deprecated")

# Force the use of the local BigFeat path
# Add the local BigFeat directory to the path with highest priority
bigfeat_local_path = r'/mnt/Projects/BigFeat'  # Update this path to your local BigFeat directory
if bigfeat_local_path not in sys.path:
    # Insert at the beginning to ensure it's found first
    sys.path.insert(0, bigfeat_local_path)
    print(f"Added local BigFeat path: {bigfeat_local_path}")

# Check to confirm which BigFeat we're using (will print its path)
bigfeat_available = False
try:
    import bigfeat

    bigfeat_path = os.path.dirname(os.path.abspath(bigfeat.__file__))
    print(f"Using BigFeat from: {bigfeat_path}")

    # Verify it's the correct version
    if bigfeat_local_path in bigfeat_path:
        print("Successfully using local BigFeat installation!")
    else:
        print(f"WARNING: Still using non-local BigFeat from {bigfeat_path}")
        print("Forcing use of local BigFeat...")
        # Try removing any previously imported module
        if 'bigfeat' in sys.modules:
            del sys.modules['bigfeat']

        # Try direct import from the specific path
        import importlib.util

        spec = importlib.util.spec_from_file_location("bigfeat",
                                                      os.path.join(bigfeat_local_path, "bigfeat", "__init__.py"))
        bigfeat = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(bigfeat)
        print(f"Re-imported BigFeat from: {os.path.dirname(os.path.abspath(bigfeat.__file__))}")

    from bigfeat.bigfeat_base import BigFeat

    bigfeat_available = True
except ImportError as e:
    print(f"ERROR importing BigFeat: {str(e)}")
    print("WARNING: BigFeat not found. Will only evaluate original features.")

# List of CSV files
csv_files = [
    '../Datasets/IOT Top 20 features datasets/device1_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device2_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device3_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device4_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device5_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device6_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device7_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device8_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device9_top_20_features.csv'
]

# Ensure output directory exists
os.makedirs('../Results/IoT/BigFeat_Ensemble_Results/', exist_ok=True)


# Create a function to evaluate ensemble models
def evaluate_ensemble_models(X_data, y_data, title, output_filename):
    """
    Evaluate different ensemble models and save performance metrics.

    Args:
        X_data: Feature DataFrame
        y_data: Target labels
        title: Title for output visualizations
        output_filename: Filename for saving metrics visualization

    Returns:
        DataFrame with performance metrics
    """
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)

    # Initialize DataFrame to store metrics
    metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

    # Bagging
    bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
    bagging_classifier.fit(X_train, y_train)
    bagging_pred = bagging_classifier.predict(X_test)

    # Stacking
    stacking_classifier = StackingClassifier(
        estimators=[('bagging', bagging_classifier)],
        final_estimator=LogisticRegression(max_iter=1000),
        stack_method='predict_proba'
    )
    stacking_classifier.fit(X_train, y_train)
    stacking_pred = stacking_classifier.predict(X_test)

    # Voting
    voting_classifier = VotingClassifier(estimators=[('bagging', bagging_classifier)], voting='hard')
    voting_classifier.fit(X_train, y_train)
    voting_pred = voting_classifier.predict(X_test)

    # Blending (Manual Implementation)
    base_learners = [bagging_classifier]
    blend_pred = np.zeros((len(X_test), len(np.unique(y_data))))  # For multiple classes
    for base_learner in base_learners:
        base_learner.fit(X_train, y_train)
        blend_pred += base_learner.predict_proba(X_test)
    blend_pred = np.argmax(blend_pred, axis=1)

    # If classes are 1-indexed, adjust predictions
    if 0 not in np.unique(y_data):
        blend_pred += 1

    # Calculate and store performance metrics
    models = ['Bagging', 'Stacking', 'Voting', 'Blending']
    preds = [bagging_pred, stacking_pred, voting_pred, blend_pred]

    for model_name, y_pred in zip(models, preds):
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        metrics_df = pd.concat([metrics_df, pd.DataFrame(
            [{'Model': model_name, 'Accuracy': accuracy, 'Precision': precision,
              'Recall': recall, 'F1 Score': f1}])], ignore_index=True)

    # Print performance metrics
    print(f"\nPerformance Metrics for {title}:")
    print(metrics_df)

    # Create visualization
    plt.figure(figsize=(10, 6))
    plt.axis('off')  # Turn off axis
    table = plt.table(
        cellText=[[f"{val:.4f}" if isinstance(val, float) else val for val in row]
                  for row in metrics_df.values],
        colLabels=metrics_df.columns,
        cellLoc='center',
        loc='center',
        colColours=['#f2f2f2'] * len(metrics_df.columns)
    )
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    plt.title(f'Ensemble Classification Performance: {title}')
    plt.tight_layout()

    # Save the table as an image
    plt.savefig(output_filename, format='jpg', dpi=300, bbox_inches='tight')
    plt.close()

    return metrics_df


# Function to add summary row to comparison dataframe
def add_summary_row(df):
    summary_row = {'Model': 'AVERAGE'}
    for col in df.columns:
        if col != 'Model':
            summary_row[col] = df[col].mean()
    return pd.concat([df, pd.DataFrame([summary_row])], ignore_index=True)


# Process each dataset one by one
for i, csv_file in enumerate(csv_files):
    print(f"\n{'=' * 80}")
    print(f"Processing device {i + 1} of {len(csv_files)}: {csv_file}")
    print(f"{'=' * 80}")
    device_name = f"device{i + 1}"

    try:
        # Load dataset
        print(f"Loading dataset {device_name}...")
        try:
            data = pd.read_csv(csv_file)
        except FileNotFoundError:
            print(f"ERROR: File not found: {csv_file}")
            continue
        except pd.errors.EmptyDataError:
            print(f"ERROR: Empty file: {csv_file}")
            continue
        except pd.errors.ParserError:
            print(f"ERROR: Parser error in file: {csv_file}")
            continue

        # Separate features and labels
        try:
            X = data.drop(columns=['label'])
            y = data['label']
        except KeyError:
            print(f"ERROR: 'label' column not found in {csv_file}")
            continue

        print(f"Dataset shape: {X.shape}")
        print(f"Label distribution: {np.unique(y, return_counts=True)}")

        # Scale features
        print(f"Scaling features for {device_name}...")
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

        # First run: evaluate performance with original features
        print(f"Evaluating ensemble models on original features for {device_name}...")
        original_output_file = f"../Results/IoT/BigFeat_Ensemble_Results/{device_name}_original_metrics.jpg"
        original_metrics = evaluate_ensemble_models(
            X_scaled_df, y,
            f"{device_name} - Original Features",
            original_output_file
        )

        # Save original metrics to CSV immediately
        original_metrics.to_csv(f"../Results/IoT/BigFeat_Ensemble_Results/{device_name}_original_metrics.csv", index=False)
        print(f"Original metrics saved to results/{device_name}_original_metrics.csv")

        # If BigFeat is not available, skip the next part
        if not bigfeat_available:
            print(f"Skipping BigFeat evaluation for {device_name} as BigFeat is not available.")
            continue

        # Initialize BigFeat for classification
        bigfeat = BigFeat(task_type='classification')

        # Generate BigFeat features with error handling
        try:
            print(f"Generating BigFeat features for {device_name}...")
            X_bigfeat = bigfeat.fit(
                X, y,
                gen_size=5,  # Number of features to generate per iteration
                random_state=42,  # For reproducibility
                iterations=3,  # Number of iterations (reduced from 5 to 3 for speed)
                estimator='rf',  # Use only RandomForest for importance
                feat_imps=True,  # Use feature importances
                check_corr=True,  # Check for correlated features
                selection='stability'  # Use stability selection
            )
        except ValueError as e:
            if "probabilities contain NaN" in str(e):
                print(f"Caught NaN probabilities error for {device_name}, using simpler approach...")
                # Fallback to a simpler configuration
                bigfeat = BigFeat(task_type='classification')
                X_bigfeat = bigfeat.fit(
                    X, y,
                    gen_size=3,
                    random_state=42,
                    iterations=2,
                    estimator='rf',  # Only use RandomForest
                    feat_imps=False,  # Turn off feature importance weighting
                    check_corr=True,
                    selection=None  # Disable stability selection
                )
            elif "No axis named None" in str(e):
                print(f"Caught axis error for {device_name}, using modified approach...")
                # Series/DataFrame axis handling issue, try with explicit axis
                bigfeat = BigFeat(task_type='classification')
                # Convert y to NumPy array to avoid Series/DataFrame axis issues
                y_array = np.array(y)
                X_bigfeat = bigfeat.fit(
                    X, y_array,
                    gen_size=4,
                    random_state=42,
                    iterations=2,
                    estimator='rf',
                    feat_imps=False,  # Disable feature importance to avoid axis issues
                    check_corr=True,
                    selection=None  # Disable stability selection
                )
            else:
                print(f"ERROR with BigFeat for {device_name}: {str(e)}")
                continue

        # Transform the data
        print(f"Transforming data with BigFeat for {device_name}...")
        try:
            X_bigfeat = bigfeat.transform(X)
        except Exception as transform_error:
            print(f"Error transforming data with BigFeat for {device_name}: {str(transform_error)}")
            # Try one more approach with a new BigFeat instance
            try:
                print("Attempting alternative transformation approach...")
                bigfeat_alt = BigFeat(task_type='classification')
                # Most basic settings possible
                X_bigfeat = bigfeat_alt.fit(
                    X.values, np.array(y),  # Use NumPy arrays
                    gen_size=2,
                    random_state=42,
                    iterations=1,
                    estimator='dt',  # Try decision tree instead
                    feat_imps=False,
                    check_corr=False,
                    selection=None
                )
                X_bigfeat = bigfeat_alt.transform(X.values)
            except Exception as alt_error:
                print(f"Alternative transformation also failed: {str(alt_error)}")
                X_bigfeat = None

        # Convert transformed features to DataFrame
        if X_bigfeat is not None:
            X_bigfeat = pd.DataFrame(X_bigfeat, columns=[f'feat_{i}' for i in range(X_bigfeat.shape[1])])
            print(f"BigFeat features shape: {X_bigfeat.shape}")

            # Second run: evaluate performance with BigFeat features
            print(f"Evaluating ensemble models with BigFeat features for {device_name}...")
            bigfeat_output_file = f"../Results/IoT/BigFeat_Ensemble_Results/{device_name}_bigfeat_metrics.jpg"
            bigfeat_metrics = evaluate_ensemble_models(
                X_bigfeat, y,
                f"{device_name} - With BigFeat Features",
                bigfeat_output_file
            )

            # Save bigfeat metrics to CSV immediately
            bigfeat_metrics.to_csv(f"../Results/IoT/BigFeat_Ensemble_Results/{device_name}_bigfeat_metrics.csv", index=False)
            print(f"BigFeat metrics saved to results/{device_name}_bigfeat_metrics.csv")

            # Compare the results
            print(f"Creating comparison for {device_name}...")
            comparison_df = pd.DataFrame()
            comparison_df['Model'] = original_metrics['Model']

            for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:
                comparison_df[f'Original {metric}'] = original_metrics[metric]
                comparison_df[f'BigFeat {metric}'] = bigfeat_metrics[metric]
                comparison_df[f'{metric} Diff'] = bigfeat_metrics[metric] - original_metrics[metric]

            # Add summary row with averages
            comparison_df = add_summary_row(comparison_df)

            # Save comparison metrics to CSV immediately
            comparison_df.to_csv(f"../Results/IoT/BigFeat_Ensemble_Results/{device_name}_comparison.csv", index=False)
            print(f"Comparison metrics saved to results/{device_name}_comparison.csv")
        else:
            print(f"Skipping BigFeat evaluation for {device_name} as feature generation failed.")

        # Save comparison metrics to CSV immediately
        comparison_df.to_csv(f"../Results/IoT/BigFeat_Ensemble_Results/{device_name}_comparison.csv", index=False)
        print(f"Comparison metrics saved to results/{device_name}_comparison.csv")

        # Create comparison visualization
        if X_bigfeat is not None:
            plt.figure(figsize=(15, 10))
            plt.axis('off')

            # Format cell text with colored diffs
            cell_text = []
            for _, row in comparison_df.iterrows():
                row_values = []
                for col in comparison_df.columns:
                    if isinstance(row[col], float):
                        # Format differences with better precision
                        value_str = f"{row[col]:.4f}"
                    else:
                        value_str = str(row[col])
                    row_values.append(value_str)
                cell_text.append(row_values)

            # Create the table
            table = plt.table(
                cellText=cell_text,
                colLabels=comparison_df.columns,
                cellLoc='center',
                loc='center',
                colColours=['#f2f2f2'] * len(comparison_df.columns)
            )

            # Style the table
            table.auto_set_font_size(False)
            table.set_fontsize(8)
            table.scale(1.5, 1.5)

            # Color code the diff cells
            for i in range(len(comparison_df)):
                for j, col in enumerate(comparison_df.columns):
                    if 'Diff' in col:
                        cell = table._cells[(i + 1, j)]
                        val = comparison_df.iloc[i][col]
                        if val > 0:
                            cell.set_facecolor('#d4f7d4')  # Light green for improvement
                        elif val < 0:
                            cell.set_facecolor('#f7d4d4')  # Light red for worse

            plt.title(f'{device_name} Comparison: Original vs BigFeat Features', fontsize=16)
            plt.tight_layout()

            # Save the comparison table
            comparison_output_file = f"../Results/IoT/BigFeat_Ensemble_Results/{device_name}_comparison.jpg"
            plt.savefig(comparison_output_file, format='jpg', dpi=300, bbox_inches='tight')
            plt.close()

            print(f"Comparison visualization saved to {comparison_output_file}")

        # Feature importance visualization
        if X_bigfeat is not None and hasattr(bigfeat, 'tracking_ids') and len(bigfeat.tracking_ids) > 0:
            try:
                from sklearn.ensemble import RandomForestClassifier

                print(f"Generating feature importance visualization for {device_name}...")

                # Train a random forest to get feature importance
                rf = RandomForestClassifier(random_state=42, n_estimators=100)
                rf.fit(X_bigfeat, y)

                # Get feature importance
                importances = rf.feature_importances_
                feature_names = X_bigfeat.columns

                # Sort features by importance
                indices = np.argsort(importances)[::-1]

                # Take top 15 or all if less
                top_n = min(15, len(indices))

                plt.figure(figsize=(10, 8))
                plt.title(f'{device_name} Feature Importances')
                plt.bar(range(top_n), importances[indices[:top_n]], align='center')
                plt.xticks(range(top_n), [feature_names[i] for i in indices[:top_n]], rotation=90)
                plt.tight_layout()

                feature_importance_file = f"results/{device_name}_feature_importance.jpg"
                plt.savefig(feature_importance_file, dpi=300, bbox_inches='tight')
                plt.close()

                print(f"Feature importance visualization saved to {feature_importance_file}")

                # Print top features
                print(f"Top features for {device_name} by importance:")
                for i in range(min(5, top_n)):  # Show top 5
                    print(f"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}")
            except Exception as e:
                print(f"Error generating feature importance for {device_name}: {str(e)}")

        print(f"Completed processing for {device_name}")
        print(f"{'=' * 80}")

    except Exception as e:
        print(f"ERROR processing {device_name}: {str(e)}")
        import traceback

        traceback.print_exc()

print("\nProcessing complete. Results are saved in the 'results' directory.")