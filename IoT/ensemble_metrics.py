#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Ensemble Learning with BigFeat Comparison

This script compares the performance of ensemble models with and without
features generated by BigFeat for IoT device datasets.
"""

import os
import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import BaggingClassifier, StackingClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import MinMaxScaler

# List of CSV files
csv_files = [
    '../Datasets/IOT Top 20 features datasets/device1_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device2_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device3_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device4_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device5_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device6_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device7_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device8_top_20_features.csv',
    '../Datasets/IOT Top 20 features datasets/device9_top_20_features.csv'
]

# Ensure output directory exists
os.makedirs('results', exist_ok=True)


# Create a function to evaluate ensemble models
def evaluate_ensemble_models(X_data, y_data, title, output_filename):
    """
    Evaluate different ensemble models and save performance metrics.

    Args:
        X_data: Feature DataFrame
        y_data: Target labels
        title: Title for output visualizations
        output_filename: Filename for saving metrics visualization

    Returns:
        DataFrame with performance metrics
    """
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.2, random_state=42)

    # Initialize DataFrame to store metrics
    metrics_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

    # Bagging
    bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)
    bagging_classifier.fit(X_train, y_train)
    bagging_pred = bagging_classifier.predict(X_test)

    # Stacking
    stacking_classifier = StackingClassifier(
        estimators=[('bagging', bagging_classifier)],
        final_estimator=LogisticRegression(max_iter=1000),
        stack_method='predict_proba'
    )
    stacking_classifier.fit(X_train, y_train)
    stacking_pred = stacking_classifier.predict(X_test)

    # Voting
    voting_classifier = VotingClassifier(estimators=[('bagging', bagging_classifier)], voting='hard')
    voting_classifier.fit(X_train, y_train)
    voting_pred = voting_classifier.predict(X_test)

    # Blending (Manual Implementation)
    base_learners = [bagging_classifier]
    blend_pred = np.zeros((len(X_test), len(np.unique(y_data))))  # For multiple classes
    for base_learner in base_learners:
        base_learner.fit(X_train, y_train)
        blend_pred += base_learner.predict_proba(X_test)
    blend_pred = np.argmax(blend_pred, axis=1)

    # If classes are 1-indexed, adjust predictions
    if 0 not in np.unique(y_data):
        blend_pred += 1

    # Calculate and store performance metrics
    models = ['Bagging', 'Stacking', 'Voting', 'Blending']
    preds = [bagging_pred, stacking_pred, voting_pred, blend_pred]

    for model_name, y_pred in zip(models, preds):
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall = recall_score(y_test, y_pred, average='weighted')
        f1 = f1_score(y_test, y_pred, average='weighted')

        metrics_df = pd.concat([metrics_df, pd.DataFrame(
            [{'Model': model_name, 'Accuracy': accuracy, 'Precision': precision,
              'Recall': recall, 'F1 Score': f1}])], ignore_index=True)

    # Print performance metrics
    print(f"\nPerformance Metrics for {title}:")
    print(metrics_df)

    # Create visualization
    plt.figure(figsize=(10, 6))
    plt.axis('off')  # Turn off axis
    table = plt.table(
        cellText=[[f"{val:.4f}" if isinstance(val, float) else val for val in row]
                  for row in metrics_df.values],
        colLabels=metrics_df.columns,
        cellLoc='center',
        loc='center',
        colColours=['#f2f2f2'] * len(metrics_df.columns)
    )
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    plt.title(f'Ensemble Classification Performance: {title}')
    plt.tight_layout()

    # Save the table as an image
    plt.savefig(output_filename, format='jpg', dpi=300, bbox_inches='tight')
    plt.close()

    return metrics_df


# Main execution
for i, csv_file in enumerate(csv_files):
    print(f"\nProcessing device {i + 1} of {len(csv_files)}: {csv_file}")
    device_name = f"device{i + 1}"

    try:
        # Load dataset
        data = pd.read_csv(csv_file)

        # Separate features and labels
        X = data.drop(columns=['label'])
        y = data['label']

        print(f"Dataset shape: {X.shape}")
        print(f"Label distribution: {np.unique(y, return_counts=True)}")

        # Scale features
        scaler = MinMaxScaler()
        X_scaled = scaler.fit_transform(X)
        X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

        # First run: evaluate performance with original features
        original_output_file = f"results/{device_name}_original_metrics.jpg"
        original_metrics = evaluate_ensemble_models(
            X_scaled_df, y,
            f"{device_name} - Original Features",
            original_output_file
        )

        # Set up BigFeat path (adjust as needed)
        bigfeat_path = r'/mnt/Projects/BigFeat'  # Parent directory of bigfeat package
        sys.path.insert(0, bigfeat_path)  # Insert at start of sys.path

        # Import BigFeat
        try:
            from bigfeat.bigfeat_base import BigFeat

            # Initialize BigFeat for classification
            bigfeat = BigFeat(task_type='classification')

            # Generate BigFeat features with error handling
            try:
                print(f"\nGenerating BigFeat features for {device_name}...")
                X_bigfeat = bigfeat.fit(
                    X, y,
                    gen_size=5,  # Number of features to generate per iteration
                    random_state=42,  # For reproducibility
                    iterations=3,  # Number of iterations (reduced from 5 to 3 for speed)
                    estimator='rf',  # Use only RandomForest for importance
                    feat_imps=True,  # Use feature importances
                    check_corr=True,  # Check for correlated features
                    selection='stability'  # Use stability selection
                )
            except ValueError as e:
                if "probabilities contain NaN" in str(e):
                    print("Caught NaN probabilities error, using simpler approach...")
                    # Fallback to a simpler configuration
                    bigfeat = BigFeat(task_type='classification')
                    X_bigfeat = bigfeat.fit(
                        X, y,
                        gen_size=3,
                        random_state=42,
                        iterations=2,
                        estimator='rf',  # Only use RandomForest
                        feat_imps=False,  # Turn off feature importance weighting
                        check_corr=True,
                        selection=None  # Disable stability selection
                    )
                else:
                    raise e

            # Transform the data
            X_bigfeat = bigfeat.transform(X)

            # Convert transformed features to DataFrame
            X_bigfeat = pd.DataFrame(X_bigfeat, columns=[f'feat_{i}' for i in range(X_bigfeat.shape[1])])

            # Second run: evaluate performance with BigFeat features
            bigfeat_output_file = f"results/{device_name}_bigfeat_metrics.jpg"
            bigfeat_metrics = evaluate_ensemble_models(
                X_bigfeat, y,
                f"{device_name} - With BigFeat Features",
                bigfeat_output_file
            )

            # Compare the results
            comparison_df = pd.DataFrame()
            comparison_df['Model'] = original_metrics['Model']

            for metric in ['Accuracy', 'Precision', 'Recall', 'F1 Score']:
                comparison_df[f'Original {metric}'] = original_metrics[metric]
                comparison_df[f'BigFeat {metric}'] = bigfeat_metrics[metric]
                comparison_df[f'{metric} Diff'] = bigfeat_metrics[metric] - original_metrics[metric]

            # Create comparison visualization
            plt.figure(figsize=(15, 10))
            plt.axis('off')

            # Format cell text with colored diffs
            cell_text = []
            for _, row in comparison_df.iterrows():
                row_values = []
                for col in comparison_df.columns:
                    if isinstance(row[col], float):
                        # Format differences with better precision
                        value_str = f"{row[col]:.4f}"
                    else:
                        value_str = str(row[col])
                    row_values.append(value_str)
                cell_text.append(row_values)

            # Create the table
            table = plt.table(
                cellText=cell_text,
                colLabels=comparison_df.columns,
                cellLoc='center',
                loc='center',
                colColours=['#f2f2f2'] * len(comparison_df.columns)
            )

            # Style the table
            table.auto_set_font_size(False)
            table.set_fontsize(8)
            table.scale(1.5, 1.5)

            # Color code the diff cells
            for i in range(len(comparison_df)):
                for j, col in enumerate(comparison_df.columns):
                    if 'Diff' in col:
                        cell = table._cells[(i + 1, j)]
                        val = comparison_df.iloc[i][col]
                        if val > 0:
                            cell.set_facecolor('#d4f7d4')  # Light green for improvement
                        elif val < 0:
                            cell.set_facecolor('#f7d4d4')  # Light red for worse

            plt.title(f'{device_name} Comparison: Original vs BigFeat Features', fontsize=16)
            plt.tight_layout()

            # Save the comparison table
            comparison_output_file = f"results/{device_name}_comparison.jpg"
            plt.savefig(comparison_output_file, format='jpg', dpi=300, bbox_inches='tight')
            plt.close()

            # Save metrics to CSV for future reference
            original_metrics.to_csv(f"results/{device_name}_original_metrics.csv", index=False)
            bigfeat_metrics.to_csv(f"results/{device_name}_bigfeat_metrics.csv", index=False)
            comparison_df.to_csv(f"results/{device_name}_comparison.csv", index=False)

            print(f"Comparison complete for {device_name}. Results saved to {comparison_output_file}")

            # Optional: Feature importance visualization
            if hasattr(bigfeat, 'tracking_ids') and len(bigfeat.tracking_ids) > 0:
                from sklearn.ensemble import RandomForestClassifier

                print(f"\nGenerating feature importance visualization for {device_name}...")

                # Train a random forest to get feature importance
                rf = RandomForestClassifier(random_state=42, n_estimators=100)
                rf.fit(X_bigfeat, y)

                # Get feature importance
                importances = rf.feature_importances_
                feature_names = X_bigfeat.columns

                # Sort features by importance
                indices = np.argsort(importances)[::-1]

                # Take top 15 or all if less
                top_n = min(15, len(indices))

                plt.figure(figsize=(10, 8))
                plt.title(f'{device_name} Feature Importances')
                plt.bar(range(top_n), importances[indices[:top_n]], align='center')
                plt.xticks(range(top_n), [feature_names[i] for i in indices[:top_n]], rotation=90)
                plt.tight_layout()
                plt.savefig(f"results/{device_name}_feature_importance.jpg", dpi=300, bbox_inches='tight')
                plt.close()

                # Print top features
                print(f"Top features for {device_name} by importance:")
                for i in range(min(5, top_n)):  # Show top 5
                    print(f"{feature_names[indices[i]]}: {importances[indices[i]]:.4f}")

        except ImportError:
            print("BigFeat not found. Please install BigFeat or adjust the path.")

    except Exception as e:
        print(f"Error processing {device_name}: {str(e)}")

print("\nProcessing complete. Results are saved in the 'results' directory.")